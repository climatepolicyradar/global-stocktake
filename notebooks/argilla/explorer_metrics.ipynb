{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import argilla as rg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import spacy\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from functools import partial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token.text for token in nlp(text)]\n",
    "\n",
    "def extract_text(row, start_col='start', end_col='end', text_col='text'):\n",
    "    if pd.isna(row[start_col]) or pd.isna(row[end_col]):\n",
    "        return np.nan\n",
    "    try:\n",
    "        start = int(row[start_col])\n",
    "        end = int(row[end_col])\n",
    "        return row[text_col][start:end]\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def create_binary_tokens(df, apply_fn, text_col='text', start_col_name='start', end_col_name='end'):\n",
    "    df['new_text'] = df.apply(apply_fn, axis=1)\n",
    "    df['text_before'] = df.apply(lambda x: x[text_col][:int(x[start_col_name])] if not math.isnan(x[start_col_name]) else np.nan, axis=1)\n",
    "    df['text_after'] = df.apply(lambda x: x[text_col][int(x[end_col_name]):] if not math.isnan(x[end_col_name]) else np.nan, axis=1)\n",
    "\n",
    "    # Creating the new tokens column\n",
    "    df['new_tokens'] = df['new_text'].apply(lambda x: tokenize(x) if pd.notnull(x) else np.nan)\n",
    "    df['tokens_before'] = df['text_before'].apply(lambda x: tokenize(x) if pd.notnull(x) else np.nan)\n",
    "    df['tokens_after'] = df['text_after'].apply(lambda x: tokenize(x) if pd.notnull(x) else np.nan)\n",
    "\n",
    "    df['in_new_span'] = df.apply(lambda x: [0]*len(x.tokens_before) + [1]*len(x.new_tokens) + [0]*len(x.tokens_after) if np.all(pd.notnull(x.new_tokens)) else np.nan, axis=1)\n",
    "    df['in_new_span'] = df['in_new_span'].fillna(df.tokens.apply(lambda x: [0]*len(x)))\n",
    "    return df\n",
    "\n",
    "def parse_annotation(annotation):\n",
    "    if isinstance(annotation, str):\n",
    "        try:\n",
    "            return ast.literal_eval(annotation)\n",
    "        except ValueError:  # catches strings that aren't list/dict syntax\n",
    "            return annotation\n",
    "    elif isinstance(annotation, float) and np.isnan(annotation):  # handle NaN values\n",
    "        return {}\n",
    "    else:\n",
    "        return annotation\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    # Concatenate all lists in the 'relevant_tokens_x' and 'relevant_tokens_y' columns\n",
    "    predictions = np.concatenate(df['predictions'].values)\n",
    "    true_values = np.concatenate(df['ground_truth'].values)\n",
    "\n",
    "    # Calculate precision, recall and F1-score\n",
    "    precision = precision_score(true_values, predictions)\n",
    "    recall = recall_score(true_values, predictions)\n",
    "    f1 = f1_score(true_values, predictions)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def process_explorer_concepts_df(explorer_df):\n",
    "    # filter the explorer_df to only include documents that are in the dataset\n",
    "    explorer_df = explorer_df[explorer_df.document_id.isin(argilla_df.document_id_new.unique())]\n",
    "\n",
    "    replace_dict = {\n",
    "        \"financial-flows\": \"Financial Flows\",\n",
    "        \"deforestation\": \"Deforestation\",\n",
    "        \"vulnerable-groups\": \"Vulnerable Groups\",\n",
    "        \"equity-and-just-transition\": \"Equity And Justice\",\n",
    "        \"barriers-and-challenges\": \"Barriers and Challenges\",\n",
    "        \"good-practice-and-opportunities\": \"Good Practices and Opportunities\",\n",
    "    }\n",
    "    explorer_df[\"subdir\"] = explorer_df[\"subdir\"].replace(replace_dict)\n",
    "\n",
    "\n",
    "\n",
    "    explorer_df['tokens'] = explorer_df['sentence'].apply(tokenize)\n",
    "\n",
    "\n",
    "\n",
    "    partial_fn = partial(extract_text, start_col='start_idx', end_col='end_idx', text_col='sentence')\n",
    "    explorer_df = create_binary_tokens(explorer_df, partial_fn, text_col='sentence', start_col_name='start_idx', end_col_name='end_idx')\n",
    "    return explorer_df\n",
    "\n",
    "def calculate_concept_metrics(explorer_df, argilla_df):\n",
    "    metrics_list=[]\n",
    "    for concept in explorer_df['subdir'].unique():\n",
    "        explorer_concept_df = explorer_df[explorer_df['subdir'] == concept]\n",
    "        argilla_concept_df = argilla_df[argilla_df['label'] == concept]\n",
    "        argilla_negatives = argilla_df[argilla_df['label'].isna()]\n",
    "        argilla_concept_df = pd.concat([argilla_concept_df, argilla_negatives], axis=0)\n",
    "\n",
    "        df_relevant_argilla = (argilla_concept_df.groupby(['text'])['in_new_span']\n",
    "                    .apply(lambda x: np.any(x.values, axis=0))\n",
    "                    .reset_index())\n",
    "        df_relevant_argilla.columns = ['text', 'ground_truth']\n",
    "\n",
    "        df_relevant_explorer = (explorer_concept_df.groupby(['sentence'])['in_new_span']\n",
    "                    .apply(lambda x: np.any(x.values, axis=0))\n",
    "                    .reset_index())\n",
    "        df_relevant_explorer.columns = ['text', 'predictions']\n",
    "\n",
    "        df_relevant_merged = df_relevant_argilla.merge(df_relevant_explorer, on='text', how='left')\n",
    "        df_relevant_merged['predictions'] = df_relevant_merged['predictions'].fillna(df_relevant_merged['ground_truth'].apply(lambda x: [0]*len(x)))\n",
    "        df_relevant_merged = df_relevant_merged[df_relevant_merged['ground_truth'].apply(lambda x: len(x)) == df_relevant_merged['predictions'].apply(lambda x: len(x))]\n",
    "\n",
    "        precision, recall, f1 = calculate_metrics(df_relevant_merged)\n",
    "        metrics_dict = {\"concept\": concept, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "        metrics_list.append(metrics_dict)\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "def process_argilla_df(df):\n",
    "    # ignore non-annotated data\n",
    "    dataset_df = df[~df[\"annotation\"].isna()]\n",
    "    # create id column for cross-referencing with explorer\n",
    "    dataset_df['document_id'] = \"CCLW\"+dataset_df.id.str.split(\"CCLW\").str[-1]\n",
    "\n",
    "    # load mapping\n",
    "    mapping = pd.read_csv(\"/home/stefan/unfccc-global-stocktake-documents/notebooks/old-to-new-dataset-mapping.csv\")\n",
    "\n",
    "\n",
    "    argilla_df=dataset_df.copy()\n",
    "\n",
    "    # assuming argilla_df is your dataframe and 'annotation' is your column\n",
    "    argilla_df['annotation'] = argilla_df['annotation'].apply(parse_annotation)\n",
    "\n",
    "    # explode the 'annotation' column\n",
    "    argilla_df = argilla_df.explode('annotation')\n",
    "\n",
    "    # create new columns from the dictionary\n",
    "    argilla_df['start'] = argilla_df['annotation'].apply(lambda d: d.get('start') if isinstance(d, dict) else np.nan)\n",
    "    argilla_df['label'] = argilla_df['annotation'].apply(lambda d: d.get('label') if isinstance(d, dict) else np.nan)\n",
    "    argilla_df['end'] = argilla_df['annotation'].apply(lambda d: d.get('end') if isinstance(d, dict) else np.nan)\n",
    "\n",
    "    # remove the 'annotation' column\n",
    "    argilla_df = argilla_df.drop(columns='annotation')\n",
    "\n",
    "    mapping.drop_duplicates(subset='document_id_old', keep='first', inplace=True)\n",
    "    argilla_df['document_id_new'] = argilla_df.document_id.map(mapping.set_index('document_id_old')['document_id_new'])\n",
    "\n",
    "    partial_fn = partial(extract_text, start_col='start', end_col='end', text_col='text')\n",
    "    argilla_df = create_binary_tokens(argilla_df, partial_fn, text_col='text', start_col_name='start', end_col_name='end')\n",
    "    return argilla_df\n",
    "\n",
    "def load_explorer_concepts_df(base_dir, concepts):\n",
    "    # File pattern\n",
    "    pattern = \"spans*.csv\"\n",
    "\n",
    "    # Store all DataFrames in a list\n",
    "    df_list = []\n",
    "\n",
    "    # Iterate over the specific subdirectories\n",
    "    for subdir in concepts:\n",
    "        dirpath = os.path.join(base_dir, subdir)\n",
    "        if os.path.exists(dirpath):  # only proceed if the directory exists\n",
    "            # Use glob to match the pattern 'spans*.csv'\n",
    "            for filename in glob.glob(os.path.join(dirpath, pattern)):\n",
    "                # Read csv file into a DataFrame and append to the list\n",
    "                sub_df = pd.read_csv(filename)\n",
    "                sub_df[\"subdir\"] = subdir\n",
    "                df_list.append(sub_df)\n",
    "        else:\n",
    "            print(f\"Directory {dirpath} does not exist.\")\n",
    "\n",
    "    # Concatenate all dataframes in the list\n",
    "    explorer_df = pd.concat(df_list, ignore_index=True)\n",
    "    return explorer_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41251/435915969.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_df['document_id'] = \"CCLW\"+dataset_df.id.str.split(\"CCLW\").str[-1]\n",
      "/tmp/ipykernel_41251/435915969.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  explorer_df[\"subdir\"] = explorer_df[\"subdir\"].replace(replace_dict)\n",
      "/tmp/ipykernel_41251/435915969.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  explorer_df['tokens'] = explorer_df['sentence'].apply(tokenize)\n",
      "/tmp/ipykernel_41251/435915969.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['new_text'] = df.apply(apply_fn, axis=1)\n",
      "/tmp/ipykernel_41251/435915969.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_before'] = df.apply(lambda x: x[text_col][:int(x[start_col_name])] if not math.isnan(x[start_col_name]) else np.nan, axis=1)\n",
      "/tmp/ipykernel_41251/435915969.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_after'] = df.apply(lambda x: x[text_col][int(x[end_col_name]):] if not math.isnan(x[end_col_name]) else np.nan, axis=1)\n",
      "/tmp/ipykernel_41251/435915969.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['new_tokens'] = df['new_text'].apply(lambda x: tokenize(x) if pd.notnull(x) else np.nan)\n",
      "/tmp/ipykernel_41251/435915969.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens_before'] = df['text_before'].apply(lambda x: tokenize(x) if pd.notnull(x) else np.nan)\n",
      "/tmp/ipykernel_41251/435915969.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens_after'] = df['text_after'].apply(lambda x: tokenize(x) if pd.notnull(x) else np.nan)\n",
      "/tmp/ipykernel_41251/435915969.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['in_new_span'] = df.apply(lambda x: [0]*len(x.tokens_before) + [1]*len(x.new_tokens) + [0]*len(x.tokens_after) if np.all(pd.notnull(x.new_tokens)) else np.nan, axis=1)\n",
      "/tmp/ipykernel_41251/435915969.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['in_new_span'] = df['in_new_span'].fillna(df.tokens.apply(lambda x: [0]*len(x)))\n",
      "/home/stefan/gstnew/global-stocktake/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "# Load spaCy model. You can also use 'en_core_web_sm' for a smaller model.\n",
    "nlp = spacy.blank(\"en\")\n",
    "DATASET_NAME = \"explorer-quality-testing\"\n",
    "# User management is done at a workspace level\n",
    "rg.init(workspace=\"gst\", api_key=os.environ[\"ARGILLA_API_KEY\"])\n",
    "dataset = rg.load(DATASET_NAME).to_datasets()\n",
    "\n",
    "dataset_df = dataset.to_pandas()\n",
    "\n",
    "\n",
    "argilla_df = process_argilla_df(dataset_df)\n",
    "\n",
    "# List of subdirectories to include\n",
    "concepts = [\n",
    "    \"financial-flows\",\n",
    "    \"deforestation\",\n",
    "    \"vulnerable-groups\",\n",
    "    \"equity-and-just-transition\",\n",
    "    \"barriers-and-challenges\",\n",
    "    \"good-practice-and-opportunities\",\n",
    "]\n",
    "\n",
    "# Base directory\n",
    "base_dir = \"/home/stefan/gst3/global-stocktake/concepts\"\n",
    "\n",
    "\n",
    "explorer_df = load_explorer_concepts_df(base_dir, concepts)\n",
    "explorer_df = process_explorer_concepts_df(explorer_df)\n",
    "\n",
    "\n",
    "metrics = calculate_concept_metrics(explorer_df, argilla_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "                            concept  precision    recall        f1\n0                   Financial Flows   0.666667  0.029851  0.057143\n1                     Deforestation   0.888889  0.017978  0.035242\n2                 Vulnerable Groups   1.000000  0.028571  0.055556\n3                Equity And Justice   0.000000  0.000000  0.000000\n4           Barriers and Challenges   1.000000  0.022305  0.043636\n5  Good Practices and Opportunities   0.666667  0.016598  0.032389",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>concept</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Financial Flows</td>\n      <td>0.666667</td>\n      <td>0.029851</td>\n      <td>0.057143</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Deforestation</td>\n      <td>0.888889</td>\n      <td>0.017978</td>\n      <td>0.035242</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Vulnerable Groups</td>\n      <td>1.000000</td>\n      <td>0.028571</td>\n      <td>0.055556</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Equity And Justice</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Barriers and Challenges</td>\n      <td>1.000000</td>\n      <td>0.022305</td>\n      <td>0.043636</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Good Practices and Opportunities</td>\n      <td>0.666667</td>\n      <td>0.016598</td>\n      <td>0.032389</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}